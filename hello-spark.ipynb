{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Spark\n",
    "\n",
    "Walkthrough of O'Reilly [Spark - The Definitive Guide](https://www.safaribooksonline.com/library/view/spark-the-definitive/9781491912201/), chapter 1\n",
    "\n",
    "To install dependencies for this notebook locally, assuming the full Spark install is in `/usr/local/spark`:\n",
    "\n",
    "```\n",
    "$ virtualenv python=python3 spark\n",
    "$ cd spark\n",
    "$ source bin/activate\n",
    "$ pip install pyspark jupyter\n",
    "$ SPARK_HOME=/usr/local/spark PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" ./bin/pyspark\n",
    "```\n",
    "\n",
    "Sample data can be downloaded from: http://cdn.oreillystatic.com/books/0636920034957/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.11:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10e054748>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple range DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_range = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, number: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_range.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_range.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0),\n",
       " Row(number=1),\n",
       " Row(number=2),\n",
       " Row(number=3),\n",
       " Row(number=4),\n",
       " Row(number=5),\n",
       " Row(number=6),\n",
       " Row(number=7),\n",
       " Row(number=8),\n",
       " Row(number=9)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_range.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evens = my_range.where(\"number % 2 = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0),\n",
       " Row(number=2),\n",
       " Row(number=4),\n",
       " Row(number=6),\n",
       " Row(number=8),\n",
       " Row(number=10),\n",
       " Row(number=12),\n",
       " Row(number=14),\n",
       " Row(number=16),\n",
       " Row(number=18)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evens.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read sample data in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/flight-data/json/2015-summary.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/flight-data/json/2015-summary.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data_json = spark.read \\\n",
    "    .json('data/flight-data/json/2015-summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*FileScan json [DEST_COUNTRY_NAME#91,ORIGIN_COUNTRY_NAME#92,count#93L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/jeffrey.sternberg/Code/spark/hello-spark/data/flight-data/json/2015..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n"
     ]
    }
   ],
   "source": [
    "flight_data_json.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data_json.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*FileScan json [DEST_COUNTRY_NAME#91,ORIGIN_COUNTRY_NAME#92,count#93L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/jeffrey.sternberg/Code/spark/hello-spark/data/flight-data/json/2015..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n"
     ]
    }
   ],
   "source": [
    "flight_data_json.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_flight_data_json = flight_data_json.sort(\"count\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Sort [count#93L ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#93L ASC NULLS FIRST, 200)\n",
      "   +- *FileScan json [DEST_COUNTRY_NAME#91,ORIGIN_COUNTRY_NAME#92,count#93L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/jeffrey.sternberg/Code/spark/hello-spark/data/flight-data/json/2015..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n"
     ]
    }
   ],
   "source": [
    "sorted_flight_data_json.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_flight_data_json.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read sample data in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight_data_csv = spark.read \\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('header', 'true')\\\n",
    "    .csv('data/flight-data/csv/2015-summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use json dataframe's schema to read csv version without inferring\n",
    "flight_data_csv = spark.read \\\n",
    "    .schema(flight_data_json.schema) \\\n",
    "    .csv('data/flight-data/csv/2015-summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep this one\n",
    "flight_data = flight_data_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert dataframe we read above to Spark SQL table/view\n",
    "flight_data.createOrReplaceTempView(\"flight_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "via_sql = spark.sql(\"\"\"\n",
    "select dest_country_name, count(*)\n",
    "from flight_data\n",
    "group by dest_country_name\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "via_df = flight_data \\\n",
    "    .groupBy('dest_country_name') \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[dest_country_name#126], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(dest_country_name#126, 200)\n",
      "   +- *HashAggregate(keys=[dest_country_name#126], functions=[partial_count(1)])\n",
      "      +- *FileScan csv [DEST_COUNTRY_NAME#126] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/jeffrey.sternberg/Code/spark/hello-spark/data/flight-data/csv/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[dest_country_name#126], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(dest_country_name#126, 200)\n",
      "   +- *HashAggregate(keys=[dest_country_name#126], functions=[partial_count(1)])\n",
      "      +- *FileScan csv [DEST_COUNTRY_NAME#126] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/jeffrey.sternberg/Code/spark/hello-spark/data/flight-data/csv/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "via_sql.explain() == via_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dest_country_name='Anguilla', count(1)=1),\n",
       " Row(dest_country_name='Russia', count(1)=1),\n",
       " Row(dest_country_name='Paraguay', count(1)=1),\n",
       " Row(dest_country_name='DEST_COUNTRY_NAME', count(1)=1),\n",
       " Row(dest_country_name='Senegal', count(1)=1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "via_sql.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) from flight_data\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "flight_data.select(max('count')).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dest_country_name='United States', dest_total=411352),\n",
       " Row(dest_country_name='Canada', dest_total=8399),\n",
       " Row(dest_country_name='Mexico', dest_total=7140),\n",
       " Row(dest_country_name='United Kingdom', dest_total=2025),\n",
       " Row(dest_country_name='Japan', dest_total=1548)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the top five destination countries in the data set?\n",
    "top_5_sql = spark.sql(\"\"\"\n",
    "select dest_country_name, sum(count) as dest_total\n",
    "from flight_data\n",
    "group by dest_country_name\n",
    "order by sum(count) desc\n",
    "limit 5\n",
    "\"\"\")\n",
    "top_5_sql.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dest_country_name='United States', dest_total=411352),\n",
       " Row(dest_country_name='Canada', dest_total=8399),\n",
       " Row(dest_country_name='Mexico', dest_total=7140),\n",
       " Row(dest_country_name='United Kingdom', dest_total=2025),\n",
       " Row(dest_country_name='Japan', dest_total=1548)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing, via dataframe\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flight_data \\\n",
    "    .groupBy(\"dest_country_name\") \\\n",
    "    .sum(\"count\") \\\n",
    "    .withColumnRenamed(\"sum(count)\", \"dest_total\") \\\n",
    "    .sort(desc(\"dest_total\")) \\\n",
    "    .limit(5) \\\n",
    "    .collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
